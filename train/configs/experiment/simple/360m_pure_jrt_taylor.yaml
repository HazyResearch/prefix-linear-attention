# @package _global_
defaults:
  - /experiment/pile/gpt3m-flash.yaml


train: 
  optimizer:
    lr: 0.0008
    betas: [0.9, 0.95]
    _target_: apex.optimizers.FusedAdam
    adam_w_mode: true
    weight_decay: 0.1
  
  scheduler: 
    lr_min: 0.00008
    _target_: src.optim.timm_lr_scheduler.TimmCosineLRScheduler
    warmup_t: 600
    t_initial: 59400
    t_in_epochs: false
    warmup_prefix: true
    warmup_lr_init: 0.000001


trainer: 
  # this interval is in terms of batch_idx not in terms of global_step, so we need 
  # to multiply by accumulate_grad_batches
  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
  max_steps: 60000

data_type: 'mixed'
enc_length: 1024
dec_length: 1024
mask_token_id: 50263
mlm_weight: 0.25
use_mlm_loss: true
loss_combination: 'partial_weighted'

datamodule:
  _target_: src.datamodules.language_modeling_neox.NeoxLMDataModule   
  batch_size: 8  # per gpu
  batch_size_eval: 8
  num_predict_batches_eval: 100
  global_batch_size: ${..train.global_batch_size}
  max_steps: ${..trainer.max_steps}
  num_test_samples: 1000
  num_valid_samples: 1000
  max_length: 2048

  use_mlm_loss: true
  enc_length: ${..enc_length}
  mask_token_id: ${..mask_token_id}

expt_name: 06-17-pure-taylor-jrt-360m-30b-fast
name: ${expt_name}


callbacks:
  model_checkpoint:
    dirpath: /home/simarora/checkpoints/${expt_name}


resume: True
do_test: False # FLAG Running Predictions


model:
  _target_: src.models.gpt.GPTLMHeadModel
  _recursive_: false
  config:
    data_type: ${...data_type}
    enc_length: ${...enc_length}
    dec_length: ${...dec_length}
    loss_combination: ${...loss_combination}

    # alt_mixer_layers: 
    #   - 1
    #   - 6
    #   - 11
    #   - 16
    #   - 21

    # alt_mixer_2_layers:
    #   - 2
    #   - 7
    #   - 12
    #   - 17
    #   - 22

    # mixer:
    #   _target_: based.models.mixers.convolution.BaseConv
    #   l_max: ${....datamodule.max_length}
    #   use_bias: True
    #   expand_proj: 4
    #   kernel_sizes: [3]

    mixer: 
      _target_: based.models.mixers.prefix_linear_attention.PrefixLinearAttention  
      l_max: ${....datamodule.max_length}
      feature_dim: 16
      feature_name: taylor_exp    
      num_heads: 16
      num_key_value_heads: 16
      enc_length: ${..enc_length}
      dec_length: ${..dec_length}
      implementation: fla_parallel

    # alt_mixer_2:
    #   _target_: based.models.mixers.slide_attention.SlidingAttention  
    #   window_size: 256
    #   num_heads: 16
    #   causal: true

    n_embd: 1152
    special_initializer: true
    n_head: 16
    n_layer: 23
    _target_: src.models.gpt.GPT2MixerConfig
    rms_norm: true
    fused_mlp: false
    attn_pdrop: 0
    embd_pdrop: 0
    n_positions: 2048
    resid_pdrop: 0
    mlp_fc1_bias: false
    mlp_fc2_bias: false
    fused_bias_fc: true
    out_proj_bias: false
    qkv_proj_bias: false
    use_flash_attn: true
    residual_in_fp32: true
    activation_function: "swiglu"    
    rotary_emb_fraction: 1           # flagging 
    fused_dropout_add_ln: true
    max_position_embeddings: 0   # flagging 
    pad_vocab_size_multiple: 8
    reorder_and_upcast_attn: false
    scale_attn_by_inverse_layer_idx: false
    n_inner: ${eval:2 * ${.n_embd}}

